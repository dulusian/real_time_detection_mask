{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is for moving the file to make into Train/Val datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOMASK = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\NMFD\\successful\" # no mask\n",
    "PMASK = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\PMFD\\successful\"  # Properly masked\n",
    "IPMASK = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\IMFD\\successful\"       # inproperly masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {NOMASK: 0, PMASK: 1, IPMASK: 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrainNM5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\\train\\NMFD\"\n",
    "newTrainIM5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\\train\\IMFD\"\n",
    "newTrainPM5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\\train\\PMFD\"\n",
    "\n",
    "newTrainNM = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\\train\\NMFD\"\n",
    "newTrainIM = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\\train\\IMFD\"\n",
    "newTrainPM = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\\train\\PMFD\"\n",
    "\n",
    "newValNM5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\\val\\NMFD\"\n",
    "newValIM5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\\val\\IMFD\"\n",
    "newValPM5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\\val\\PMFD\"\n",
    "\n",
    "newValNM = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\\val\\NMFD\"\n",
    "newValIM = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\\val\\IMFD\"\n",
    "newValPM = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\\val\\PMFD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = newTrainPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To move the file into Train/Val set'''\n",
    "def moveFile(flag=False):\n",
    "    trainCount = 0\n",
    "    valCount = 0\n",
    "    for dirName, subdirList, fileList in os.walk(label):\n",
    "        for f in fileList:\n",
    "            if f.endswith(\".png\") or f.endswith(\".jpg\"):\n",
    "                if trainCount < 5000:\n",
    "                    path = os.path.join(dirName,f)\n",
    "                    newPath = os.path.join(newTrainPM5000,f)\n",
    "                    copyfile(path, newPath)\n",
    "                    trainCount += 1\n",
    "                if trainCount == 5000 and valCount < 1000:\n",
    "                    path = os.path.join(dirName,f)\n",
    "                    newPath = os.path.join(newValPM5000,f)\n",
    "                    copyfile(path, newPath)\n",
    "                    valCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data augmentation and normalization for training'''\n",
    "'''Just normalization for validation'''\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "Mask5000 = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask5000\"\n",
    "MaskAll = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMask\"\n",
    "MaskTwoClass = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\FaceMaskTwoClass\"\n",
    "\n",
    "dataDir = MaskAll\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(dataDir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=10,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "print(inputs.shape)\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "#imshow(out, title=[x for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, model, dataloaders, criterion, optimizer, scheduler, num_epochs):\n",
    "        self.EPOCHS = num_epochs\n",
    "        self.dataloader = dataloaders\n",
    "    #     self.IMG_HEIGHT = IMG_HEIGHT\n",
    "    #     self.IMG_WIDTH  = IMG_WIDTH\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        since = time.time()\n",
    "\n",
    "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            print('Epoch {}/{}'.format(epoch, self.EPOCHS - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    self.model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    self.model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in self.dataloader[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = self.criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            self.optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, img):\n",
    "        with torch.no_grad():\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            X_pred = torch.Tensor(img).view(-1,self.IMG_HEIGHT, self.IMG_WIDTH)\n",
    "            X_pred = X_pred/255\n",
    "            X_pred = X_pred.view(-1, 1, self.IMG_HEIGHT, self.IMG_WIDTH)\n",
    "            X_pred = X_pred.to(device)        \n",
    "            outputs = self.model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            prediction = torch.argmax(outputs)\n",
    "            if prediction == 0:\n",
    "                print('No mask',outputs[0])\n",
    "                return  0\n",
    "            elif prediction == 1:\n",
    "                print('Properly weared mask',outputs[0])  \n",
    "                return  1\n",
    "            else:\n",
    "                print('Inproperly weared mask',outputs[0])     \n",
    "                return  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resModel = models.resnet18(pretrained=True)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "densenet = models.densenet161(pretrained=True)\n",
    "googlenet = models.googlenet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resModel.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resModel = models.resnet18(pretrained=True)\n",
    "num_ftrs = resModel.fc.in_features\n",
    "resModel.fc = nn.Linear(num_ftrs, 2)\n",
    "resModel = resModel.to(device)\n",
    "optimizer = optim.Adam(resModel.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "# num_ftrs = alexnet.fc.in_features\n",
    "# alexnet.fc = nn.Linear(num_ftrs, 3)\n",
    "alexnet = alexnet.to(device)\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "# num_ftrs = squeezenet.fc.in_features\n",
    "# squeezenet.fc = nn.Linear(num_ftrs, 3)\n",
    "squeezenet = squeezenet.to(device)\n",
    "optimizer = optim.Adam(squeezenet.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16 = vgg16.to(device)\n",
    "optimizer = optim.Adam(vgg16.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet161(pretrained=True)\n",
    "densenet = densenet.to(device)\n",
    "optimizer = optim.Adam(densenet.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = models.inception_v3(pretrained=True)\n",
    "inception = inception.to(device)\n",
    "optimizer = optim.Adam(inception.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet = models.googlenet(pretrained=True)\n",
    "googlenet = googlenet.to(device)\n",
    "num_ftrs = googlenet.fc.in_features\n",
    "googlenet.fc = nn.Linear(num_ftrs, 3)\n",
    "optimizer = optim.Adam(googlenet.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleModel = Model(googlenet, dataloaders, criterion, optimizer ,exp_lr_scheduler, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPATH = r\"C:\\My Folder\\Data Science\\Fall 2020\\Deep Learning\\Project\\GoogleNetWholeModel.pth\"\n",
    "model = torch.load(modelPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleModel = Model(model, dataloaders, criterion, optimizer ,exp_lr_scheduler, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = exampleModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect_crop(img, classifier, scaleFactor, minNeighbors, min_size, max_size):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors, minSize=min_size, maxSize=max_size)\n",
    "    detected = True\n",
    "    if len(features) == 0:\n",
    "        x = 0\n",
    "        y = 0\n",
    "        w = gray_img.shape[1]\n",
    "        h = gray_img.shape[0]\n",
    "        features = [[x, y, w, h]]\n",
    "        cropped = False\n",
    "    coords = []\n",
    "    for (x, y, w, h) in features:\n",
    "        coords = [x, y, w, h]\n",
    "    \n",
    "    cropped_img = img[y:y+h, x:x+h].copy()\n",
    "    return coords, cropped_img, detected\n",
    "\n",
    "\n",
    "def prediction(cropped_img, color, model):    #, model\n",
    "    transformed_img = preprocessImage(cropped_img)    \n",
    "    outputs = model(transformed_img.to(device))\n",
    "    prediction = torch.argmax(outputs)\n",
    "    \n",
    "    if prediction == 0: \n",
    "        color = color['red']\n",
    "        text = \"Improperly weared mask detected\"\n",
    "        \n",
    "    elif prediction == 1:\n",
    "        color = color['green']\n",
    "        text = \"No mask detected\"\n",
    "    else:\n",
    "        color = color['blue']\n",
    "        text = \"Properly weared mask detected\"\n",
    "        \n",
    "              \n",
    "    return color, text\n",
    "\n",
    "\n",
    "def preprocessImage(img):\n",
    "    transformer = transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    inp = torch.from_numpy(img.transpose((2, 0, 1)))\n",
    "    inp = F.to_pil_image(inp)\n",
    "    inp = transformer(inp)\n",
    "    transformed_img = inp.unsqueeze(0)\n",
    "    return transformed_img\n",
    "\n",
    "\n",
    "def draw_boundary(img, coords, color, text):\n",
    "    for (x, y, w, h) in coords:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), color,2)\n",
    "        cv2.putText(img, text, (x, y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
